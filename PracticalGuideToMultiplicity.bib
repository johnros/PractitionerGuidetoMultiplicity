
@book{lazar_statistical_2008,
	edition = {1},
	title = {The Statistical Analysis of Functional {MRI} Data},
	isbn = {0387781900},
	publisher = {Springer},
	author = {Lazar, Nicole A.},
	month = jul,
	year = {2008}
},

@misc{r_development_core_team_r:_2011,
	title = {R: A Language and Environment for Statistical Computing},
	url = {http://www.R-project.org},
	urldate = {2011-05-01},
	author = {{R Development Core Team}},
	year = {2011},
	howpublished = {{http://www.R-project.org}},
	file = {R FAQ:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/8PKBMGVJ/R-FAQ.html:text/html}
},

@article{efron_microarrays_2008,
	title = {Microarrays, empirical Bayes and the two-groups model},
	volume = {23},
	number = {1},
	author = {Efron, B.},
	year = {2008},
	keywords = {Bayesian Inference, {FDR}, Microarray},
	pages = {1--22},
	file = {Google Scholar Linked Page:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/TVFE4XQ7/DPubS.html:text/html}
},

@article{benjamini_john_2002,
	title = {John W. Tukey's Contributions to Multiple Comparisons},
	volume = {30},
	issn = {00905364},
	url = {http://www.jstor.org/stable/1558730},
	doi = {10.2307/1558730},
	abstract = {This article provides a historical overview of the philosophical, theoretical and practical contributions made by John Tukey to the field of simultaneous inference. His early work, culminating in the monograph {"The} Problem of Multiple Comparisons," established him as one of the pioneers in the field, investing it with both academic respectability and a focus on practical problems. For many years afterward, Tukey only published sporadically in the area but remained convinced that multiplicity issues were of fundamental importance. During the last decade of his life, Tukey again devoted substantial attention to multiplicity, experimenting with different graphical representations of multiple comparison procedures and exploring the implications of new approaches to controlling family-wise error rates. He leaves a rich legacy that should engage and inspire statisticians for many years to come.},
	number = {6},
	urldate = {2009-07-01},
	author = {Benjamini, Yoav and Braun, Henry},
	month = dec,
	year = {2002},
	note = {{ArticleType:} primary\_article / Full publication date: Dec., 2002 / Copyright {\textcopyright} 2002 Institute of Mathematical Statistics},
	keywords = {People},
	pages = {1576--1594},
	file = {euclid.aos.1043351247.pdf:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/2N47AVA4/euclid.aos.1043351247.pdf:application/pdf;John W. Tukey's Contributions to Multiple Comparisons:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/58JREVRC/1558730.html:text/html;JSTOR Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/BGPS27N9/Benjamini and Braun - 2002 - John W Tukey's Contributions to Multiple Comparis.pdf:application/pdf}
},

@article{storey_direct_2002,
	title = {A direct approach to false discovery rates},
	volume = {64},
	url = {http://dx.doi.org/10.1111/1467-9868.00346},
	doi = {10.1111/1467-9868.00346},
	abstract = {{Summary.Multiple-hypothesis} testing involves guarding against much more complicated errors than single-hypothesis testing. Whereas we typically control the type I error rate for a single-hypothesis test, a compound error rate is controlled for multiple-hypothesis tests. For example, controlling the false discovery rate {FDR} traditionally involves intricate sequential p-value rejection methods based on the observed data. Whereas a sequential p-value method fixes the error rate and estimates its corresponding rejection region, we propose the opposite approach2014we fix the rejection region and then estimate its corresponding error rate. This new approach offers increased applicability, accuracy and power. We apply the methodology to both the positive false discovery rate {pFDR} and {FDR}, and provide evidence for its benefits. It is shown that {pFDR} is probably the quantity of interest over {FDR.} Also discussed is the calculation of the q-value, the {pFDR} analogue of the p-value, which eliminates the need to set the error rate beforehand as is traditionally done. Some simple numerical examples are presented that show that this new approach can yield an increase of over eight times in power compared with the {Benjamini2013Hochberg} {FDR} method.},
	number = {3},
	urldate = {2009-02-23},
	author = {Storey, John D.},
	year = {2002},
	keywords = {False discovery rate, {FDR}, Multiple comparisons, {pFDR}, Positive false discovery rate, p-values, qvalue, q-values, Sequential p-value methods, Simultaneous inference, Statistics},
	pages = {479--498},
	file = {Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/UMBWJ8RR/Storey - 2002 - A direct approach to false discovery rates.pdf:application/pdf;Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/7N5I84RV/full.html:text/html;Wiley Interscience PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/EAI7QWEU/fulltext.pdf:application/pdf}
},

@article{chumbley_false_2009,
	title = {False discovery rate revisited: {FDR} and topological inference using Gaussian random fields},
	volume = {44},
	issn = {1053-8119},
	shorttitle = {False discovery rate revisited},
	url = {http://www.sciencedirect.com/science/article/B6WNP-4SK62WS-1/2/926fab31b07ef0f8ee800ad5e0a0448b},
	doi = {10.1016/j.neuroimage.2008.05.021},
	abstract = {In this note, we revisit earlier work on false discovery rate ({FDR)} and evaluate it in relation to topological inference in statistical parametric mapping. We note that controlling the false discovery rate of voxels is not equivalent to controlling the false discovery rate of activations. This is a problem that is unique to inference on images, in which the underlying signal is continuous (i.e., signal which does not have a compact support). In brief, inference based on conventional voxel-wise {FDR} procedures is not appropriate for inferences on the topological features of a statistical parametric map ({SPM)}, such as peaks or regions of activation. We describe the nature of the problem, illustrate it with some examples and suggest a simple solution based on controlling the false discovery rate of connected excursion sets within an {SPM}, characterised by their volume.},
	number = {1},
	urldate = {2009-01-13},
	author = {Chumbley, Justin R. and Friston, Karl J.},
	month = jan,
	year = {2009},
	keywords = {{FDR}, {MRI}, Random Fields},
	pages = {62--70},
	file = {ScienceDirect Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/T52BRPS5/Chumbley and Friston - 2009 - False discovery rate revisited FDR and topologica.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/8KA9PVES/science.html:text/html}
},

@article{tukey_philosophy_1991,
	title = {The Philosophy of Multiple Comparisons},
	volume = {6},
	url = {http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.ss/1177011945&page=record},
	abstract = {This paper is based on the 1989 Miller Memorial Lecture at Stanford University. The topic was chosen because of Rupert Miller's long involvement and significant contributions to multiple comparison procedures and theory. Our emphasis will be on the major questions that have received relatively little attention--on what one wants multiple comparisons to do, on why one wants to do that, and on how one can communicate the results. Very little attention will be given to how the results can be calculated--after all, there are books about that (e.g., Miller, 1966, 1981; Hochberg and Tamhane, 1987).},
	number = {1},
	urldate = {2009-07-01},
	author = {Tukey, John W.},
	year = {1991},
	keywords = {Philosophy, Statistics},
	pages = {100--116},
	file = {Euclid Project PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/MHJUPEID/DPubS.html:text/html;euclid.ss.1177011945.pdf:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/D5HP4Q8Z/euclid.ss.1177011945.pdf:application/pdf}
},

@article{sun_oracle_2007,
	title = {Oracle and Adaptive Compound Decision Rules for False Discovery Rate Control},
	volume = {102},
	issn = {0162-1459, 1537-{274X}},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214507000000545},
	doi = {10.1198/016214507000000545},
	number = {479},
	urldate = {2012-07-04},
	author = {Sun, Wenguang and Cai, T. Tony},
	month = sep,
	year = {2007},
	pages = {901--912},
	file = {American Statistical Association Portal :: Oracle and Adaptive Compound Decision Rules for False Discovery Rate Control - Journal of the American Statistical Association - Volume 102, Issue 479:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/QMG8JTA3/016214507000000545.html:text/html}
},

@article{stein_voxelwise_2010,
	title = {Voxelwise genome-wide association study ({vGWAS)}},
	volume = {53},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811910002004},
	doi = {10.1016/j.neuroimage.2010.02.032},
	abstract = {The structure of the human brain is highly heritable, and is thought to be influenced by many common genetic variants, many of which are currently unknown. Recent advances in neuroimaging and genetics have allowed collection of both highly detailed structural brain scans and genome-wide genotype information. This wealth of information presents a new opportunity to find the genes influencing brain structure. Here we explore the relation between 448,293 single nucleotide polymorphisms in each of 31,622 voxels of the entire brain across 740 elderly subjects (mean age\&\#xa0;{\^A}{\textpm}\&\#xa0;s.d.: 75.52\&\#xa0;{\^A}{\textpm}\&\#xa0;6.82{\^A}~years; 438 male) including subjects with Alzheimer's disease, Mild Cognitive Impairment, and healthy elderly controls from the Alzheimer's Disease Neuroimaging Initiative ({ADNI).} We used tensor-based morphometry to measure individual differences in brain structure at the voxel level relative to a study-specific template based on healthy elderly subjects. We then conducted a genome-wide association at each voxel to identify genetic variants of interest. By studying only the most associated variant at each voxel, we developed a novel method to address the multiple comparisons problem and computational burden associated with the unprecedented amount of data. No variant survived the strict significance criterion, but several genes worthy of further exploration were identified, including {CSMD2} and {CADPS2.} These genes have high relevance to brain structure. This is the first voxelwise genome wide association study to our knowledge, and offers a novel method to discover genetic influences on brain structure.},
	number = {3},
	urldate = {2011-11-13},
	author = {Stein, Jason L. and Hua, Xue and Lee, Suh and Ho, April J. and Leow, Alex D. and Toga, Arthur W. and Saykin, Andrew J. and Shen, Li and Foroud, Tatiana and Pankratz, Nathan and Huentelman, Matthew J. and Craig, David W. and Gerber, Jill D. and Allen, April N. and Corneveaux, Jason J. and {DeChairo}, Bryan M. and Potkin, Steven G. and Weiner, Michael W. and M. Thompson, Paul},
	month = nov,
	year = {2010},
	pages = {1160--1174},
	file = {ScienceDirect Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/NKKA5CZJ/Stein et al. - 2010 - Voxelwise genome-wide association study (vGWAS).pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/KVWMHVHD/S1053811910002004.html:text/html}
},

@article{benjamini_adjusting_2011,
	title = {Adjusting for selection bias in testing multiple families of hypotheses},
	url = {http://arxiv.org/abs/1106.3670},
	abstract = {In many large multiple testing problems the hypotheses are divided into families. Given the data, families with evidence for true discoveries are selected, and hypotheses within them are tested. Neither controlling the error-rate in each family separately nor controlling the error-rate over all hypotheses together can assure that an error-rate is controlled in the selected families. We formulate this concern about selective inference in its generality, for a very wide class of error-rates and for any selection criterion, and present an adjustment of the testing level inside the selected families that retains the average error-rate over the selected families.},
	urldate = {2011-11-13},
	author = {Benjamini, Yoav and Bogomolov, Marina},
	month = jun,
	year = {2011},
	keywords = {Mathematics - Statistics Theory},
	file = {1106.3670 PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/D3X5UR93/Benjamini and Bogomolov - 2011 - Adjusting for selection bias in testing multiple f.pdf:application/pdf;arXiv.org Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/DFI9I8N2/1106.html:text/html}
},

@article{chumbley_topological_2010,
	title = {Topological {FDR} for neuroimaging},
	volume = {49},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811909012245},
	doi = {10.1016/j.neuroimage.2009.10.090},
	abstract = {In this technical note, we describe and validate a topological false discovery rate ({FDR)} procedure for statistical parametric mapping. This procedure is designed to deal with signal that is continuous and has, in principle, unbounded spatial support. We therefore infer on topological features of the signal, such as the existence of local maxima or peaks above some threshold. Using results from random field theory, we assign a p-value to each maximum in an {SPM} and identify an adaptive threshold that controls false discovery rate, using the Benjamini and Hochberg ({BH)} procedure (1995). This provides a natural complement to conventional family wise error ({FWE)} control on local maxima. We use simulations to contrast these procedures; both in terms of their relative number of discoveries and their spatial accuracy (via the distribution of the Euclidian distance between true and discovered activations). We also assessed two other procedures: cluster-wise and voxel-wise {FDR} procedures. Our results suggest that (a) {FDR} control of maxima or peaks is more sensitive than {FWE} control of peaks with minimal cost in terms of false-positives, (b) voxel-wise {FDR} is substantially less accurate than topological {FWE} or {FDR} control. Finally, we present an illustrative application using an {fMRI} study of visual attention.},
	number = {4},
	urldate = {2012-01-16},
	author = {Chumbley, J. and Worsley, K. and Flandin, G. and Friston, K.},
	month = feb,
	year = {2010},
	keywords = {{FDR}, {fMRI}, Random Fields},
	pages = {3057--3064},
	file = {ScienceDirect Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/KC5SMCQT/Chumbley et al. - 2010 - Topological FDR for neuroimaging.pdf:application/pdf;ScienceDirect Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/VR5768VR/Chumbley et al. - 2010 - Topological FDR for neuroimaging:}
},

@article{siegmund_false_2011,
	title = {False discovery rate for scanning statistics},
	volume = {98},
	url = {http://biomet.oxfordjournals.org/content/98/4/979.abstract},
	doi = {10.1093/biomet/asr057},
	abstract = {The false discovery rate is a criterion for controlling Type I error in simultaneous testing of multiple hypotheses. For scanning statistics, due to local dependence, clusters of neighbouring hypotheses are likely to be rejected together. In such situations, it is more intuitive and informative to group neighbouring rejections together and count them as a single discovery, with the false discovery rate defined as the proportion of clusters that are falsely declared among all declared clusters. Assuming that the number of false discoveries, under this broader definition of a discovery, is approximately Poisson and independent of the number of true discoveries, we examine approaches for estimating and controlling the false discovery rate, and provide examples from biological applications.},
	number = {4},
	urldate = {2012-01-24},
	author = {Siegmund, D. O. and Zhang, N. R. and Yakir, B.},
	month = dec,
	year = {2011},
	keywords = {{FDR}, {fMRI}, Martingales, Poisson Clumping},
	pages = {979 --985},
	file = {Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/5QT3IEJX/Siegmund et al. - 2011 - False discovery rate for scanning statistics.pdf:application/pdf;Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/E6AK5N78/979.html:text/html}
},

@article{benjamini_discovering_2010,
	title = {Discovering the false discovery rate},
	volume = {72},
	issn = {13697412},
	doi = {10.1111/j.1467-9868.2010.00746.x},
	abstract = {I describe the background for the paper {'Controlling} the false discovery rate: a new and powerful approach to multiple comparisons' by Benjamini and Hochberg that was published in the Journal of the Royal Statistical Society, Series B, in 1995. I review the progress since made on the false discovery rate, as well as the major conceptual developments that followed. {\textcopyright} 2010 Royal Statistical Society.},
	language = {English},
	number = {4},
	author = {Benjamini, Y.},
	year = {2010},
	keywords = {False coverage rate, Multiple comparisons, {'Testimation'}},
	pages = {405--416}
},

@article{benjamini_controlling_1995,
	title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
	volume = {57},
	copyright = {Copyright {\textcopyright} 1995 Royal Statistical Society},
	issn = {0035-9246},
	shorttitle = {Controlling the False Discovery Rate},
	url = {http://www.jstor.org/stable/2346101},
	doi = {10.2307/2346101},
	abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate ({FWER).} This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the {FWER} when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the {FWER} is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
	number = {1},
	urldate = {2013-03-07},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Benjamini, Yoav and Hochberg, Yosef},
	month = jan,
	year = {1995},
	note = {{ArticleType:} research-article / Full publication date: 1995 / Copyright {\textcopyright} 1995 Royal Statistical Society},
	pages = {289--300},
	file = {JSTOR Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/D5FV8QSI/Benjamini and Hochberg - 1995 - Controlling the False Discovery Rate A Practical .pdf:application/pdf}
},

@article{farcomeni_review_2008,
	title = {A review of modern multiple hypothesis testing, with particular attention to the false discovery proportion},
	volume = {17},
	issn = {0962-2802, 1477-0334},
	url = {http://smm.sagepub.com/content/17/4/347},
	doi = {10.1177/0962280206079046},
	abstract = {In the last decade a growing amount of statistical research has been devoted to multiple testing, motivated by a variety of applications in medicine, bioinformatics, genomics, brain imaging, etc. Research in this area is focused on developing powerful procedures even when the number of tests is very large. This paper attempts to review research in modern multiple hypothesis testing with particular attention to the false discovery proportion, loosely defined as the number of false rejections divided by the number of rejections. We review the main ideas, stepwise and augmentation procedures; and resampling based testing. We also discuss the problem of dependence among the test statistics. Simulations make a comparison between the procedures and with Bayesian methods. We illustrate the procedures in applications in {DNA} microarray data analysis. Finally, few possibilities for further research are highlighted.},
	language = {en},
	number = {4},
	urldate = {2013-03-07},
	journal = {Stat Methods Med Res},
	author = {Farcomeni, Alessio},
	month = aug,
	year = {2008},
	pages = {347--388},
	file = {Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/T6C2J7W3/347.html:text/html}
},

@article{genovese_exceedance_2006,
	title = {Exceedance Control of the False Discovery Proportion},
	volume = {101},
	issn = {0162-1459, 1537-{274X}},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000339},
	doi = {10.1198/016214506000000339},
	number = {476},
	urldate = {2013-03-07},
	author = {Genovese, Christopher R and Wasserman, Larry},
	month = dec,
	year = {2006},
	pages = {1408--1417},
	file = {American Statistical Association Portal :: Exceedance Control of the False Discovery Proportion - Journal of the American Statistical Association - Volume 101, Issue 476:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/7X7Q2C93/016214506000000339.html:text/html}
},

@article{van_der_laan_augmentation_2004,
	title = {Augmentation procedures for control of the generalized family-wise error rate and tail probabilities for the proportion of false positives},
	volume = {3},
	issn = {1544-6115},
	doi = {10.2202/1544-6115.1042},
	abstract = {This article shows that any single-step or stepwise multiple testing procedure (asymptotically) controlling the family-wise error rate ({FWER)} can be augmented into procedures that (asymptotically) control tail probabilities for the number of false positives and the proportion of false positives among the rejected hypotheses. Specifically, given any procedure that (asymptotically) controls the {FWER} at level alpha, we propose simple augmentation procedures that provide (asymptotic) level-alpha control of: (i) the generalized family-wise error rate, i.e., the tail probability, {gFWER(k)}, that the number of Type I errors exceeds a user-supplied integer k, and (ii) the tail probability, {TPPFP(q)}, that the proportion of Type I errors among the rejected hypotheses exceeds a user-supplied value 0{\textless}q{\textless}1. Existing approaches for control of the proportion of false positives typically rely on the assumption that the test statistics are independent, while our proposed augmentation procedures control the {gFWER} and {TPPFP} for general data generating distributions, with arbitrary dependence structures among variables. Applying the augmentation methods to step-down multiple testing procedures that control the {FWER} asymptotically exactly at level alpha (van der Laan et al., 2004), yields procedures that also provide exact asymptotic control of the {gFWER} and {TPPFP} at level alpha. The adjusted p-values for the {gFWER} and {TPPFP-controlling} augmentation procedures are shown to be simple functions of the adjusted p-values for the original {FWER-controlling} procedure. Finally, two simple conservative procedures are proposed for controlling the false discovery rate.},
	journal = {Stat Appl Genet Mol Biol},
	author = {van der Laan, Mark J and Dudoit, Sandrine and Pollard, Katherine S},
	year = {2004},
	note = {{PMID:} 16646793},
	pages = {Article15}
},

@article{benjamini_simultaneous_2010,
	title = {Simultaneous and selective inference: Current successes and future challenges},
	volume = {52},
	copyright = {Copyright {\textcopyright} 2010 {WILEY-VCH} Verlag {GmbH} \& Co. {KGaA}, Weinheim},
	issn = {1521-4036},
	shorttitle = {Simultaneous and selective inference},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/bimj.200900299/abstract},
	doi = {10.1002/bimj.200900299},
	abstract = {The previous decade can be viewed as a second golden for era Multiple Comparisons research. I argue that much of the success stems from our being able to address real current needs. At the same time, this success generated a plethora of concepts for error rate and power, as well as multiplicity of methods for addressing them. These confuse the users of our methodology and pose a threat. To avoid the threat, it is our responsibility to match our theoretical goals to the goals of the users of statistics. Only then should we match the methods to the theoretical goals. Considerations related to such needs are discussed: simultaneous inference or selective inference, testing or estimation, decision making or scientific reporting. I then further argue that the vitality of our field in the future {\textendash} as a research area {\textendash} depends upon our ability to continue and address the real needs of statistical analyses in current problems. Two application areas offering new challenges have received less attention in our community to date are discussed. Safety analysis in clinical trials, where I offer an aggregated safety assessment methodology and functional Magnetic Resonance Imaging.},
	language = {en},
	number = {6},
	urldate = {2013-03-11},
	author = {Benjamini, Yoav},
	year = {2010},
	keywords = {Aggregated safety analysis, False discovery rates, Familywise error rate, Functional Magnetic Resonance Imaging, Multiple comparisons procedures},
	pages = {708{\textendash}721},
	file = {Full Text PDF:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/HP8FT2XQ/Benjamini - 2010 - Simultaneous and selective inference Current succ.pdf:application/pdf;Snapshot:/home/johnros/.mozilla/firefox/uq3b2z5c.default/zotero/storage/IJN4FMNA/full.html:text/html}
},

@book{westfall_multiple_2011,
	title = {Multiple Comparisons and Multiple Tests Using {SAS}, Second Edition},
	isbn = {9781607648857},
	abstract = {New and extensively updated for {SAS} 9 and later! Have you ever felt that there was no multiple inference method that fit the particular constraints of your data? Or been overwhelmed by the many choices of procedures? Multiple Comparisons and Multiple Tests Using {SAS}, Second Edition, written by Peter Westfall, Randall Tobias, and Russell Wolfinger, solves both problems for you by providing cutting-edge methods, specialized macros, and proven "best bet" procedures. The specialized macros and dozens of real-world examples illustrate solutions for a broad variety of problems that call for multiple inferences. The book also discusses the pitfalls and advantages of various methods, thereby helping you decide which is the most appropriate for your purposes. If you are a researcher or scientist in pharmaceuticals, engineering, government, or medicine, you will find many methods applied to real data and examples from your field. The book includes specialized code and explanations throughout. It discusses in detail pairwise comparisons and comparisons with a control. Additional topics include general linear contrasts; multiple comparisons of multivariate means; and multiple inferences with mixed models, discrete data, and survival analysis.},
	language = {en},
	publisher = {{SAS} Institute},
	author = {Westfall, Peter H. and Tobias, R. Randall Davis and Wolfinger, Russell Dean},
	month = aug,
	year = {2011},
	keywords = {Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / General}
},

@article{donoho_higher_2004,
	title = {Higher criticism for detecting sparse heterogeneous mixtures},
	volume = {32},
	issn = {0090-5364},
	doi = {10.1214/009053604000000265},
	abstract = {Higher criticism, or second-level significance testing, is a multiple-comparisons concept mentioned in passing by Tukey. It concerns a situation where there are many independent tests of significance and one is interested in rejecting the joint null hypothesis. Tukey suggested comparing the fraction of observed significances at a given alpha-level to the expected fraction under the joint null. In fact, he suggested standardizing the difference of the two quantities and forming a z-score; the resulting z-score tests the significance of the body of significance tests.   We consider a generalization, where we maximize this z-score over a range of significance levels 0 {\textless} \&alpha; {\&LE;} \&alpha;(0). We are able to show that the resulting higher critic-ism statistic is effective at resolving a very subtle testing problem: testing whether n normal means are all zero versus the alternative that a small fraction is nonzero.   The subtlety of this "sparse normal means" testing problem can be seen from work of Ingster and Jin, who studied such problems in great detail. In their Studies, they identified an interesting range of cases where the small fraction of nonzero means is so small that the alternative hypothesis exhibits little noticeable effect on the distribution of the p-values either for the bulk of the tests or for the few most highly significant tests. In this range, when the amplitude of nonzero means is calibrated with the fraction of nonzero means, the likelihood ratio test for a precisely specified alternative would still succeed in separating the two hypotheses.   We show that the higher criticism is successful throughout the same region of amplitude sparsity where the likelihood ratio test would succeed. Since it does not require a specification of the alternative, this shows that higher criticism is in a sense optimally adaptive to unknown sparsity and size of the nonnull effects. While our theoretical work is largely asymptotic, we provide Simulations in finite samples and suggest some possible applications. We also show that higher critcism works well over a range of non-Gaussian cases.},
	language = {English},
	number = {3},
	journal = {Ann. Stat.},
	author = {Donoho, D. and Jin, J. S.},
	month = jun,
	year = {2004},
	note = {{WOS:000221981400005}},
	pages = {962--994}
},

@book{poldrack_handbook_2011,
	edition = {1},
	title = {Handbook of Functional {MRI} Data Analysis},
	isbn = {0521517664},
	publisher = {Cambridge University Press},
	author = {Poldrack, Russell A. and Mumford, Jeanette A. and Nichols, Thomas E.},
	month = aug,
	year = {2011}
}