\documentclass[review,12pt]{article}
%\documentclass[12pt,draft]{amsart}
%\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{amsfonts}   % if you want the fonts
\usepackage{epstopdf}
\usepackage{setspace}
\usepackage[numbers]{natbib}
%\bibpunct{[}{]}{,}{n}{}{;}
\usepackage{hyperref}
%\usepackage{glossaries}
\usepackage{float}



\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\doublespacing
%\makeglossaries


\title{A Practitioner's Guide to Multiple Testing Error Rates}
\author{Jonathan Rosenblatt}
\date{}                                         




\begin{document}
\maketitle


\section{Introduction}

It is quite common for modern research to test many hypotheses simultaneously. The frequentist hypothesis testing framework does not scale with the number of hypotheses in the sense that performing many $\alpha$ level tests will probably yield many false findings. In order to scale, a researcher has to consider of the type or errors he wishes to avoid and select the adequate procedure for that particular error type and data structure. A quick search of the tag \emph{[multiple-comparisons]} in the statistics Questions \& Answers web site Cross Validates (\url{http://stats.stackexchange.com}) demonstrates the amount of confusion this task can actually cause. This was also a point made at the 2009 Multiple Comparisons conference in Tokyo \citep[][section 4.4]{benjamini_simultaneous_2010}. 
In an attempt to offer guidance, we review possible error types for simultaneous inference (sec \ref{sec:measures_of_error}) and demonstrate them with some examples (sec \ref{sec:examples}). We also include some notes on the software implementing these procedures in appendix~\ref{sec:on_your_pc}.

The emphasis of this manuscript is on the error rates, and not on the procedures themselves. For the purpose of selecting the appropriate procedure consult your favourite software's documentation (see our appendix~\ref{sec:on_your_pc}). Alternatively, \citet{farcomeni_review_2008} can serve as a good reference. 
We do try to name several procedures in this manuscript where appropriate.  Simultaneous confidence intervals and p-value adjustment will not be discussed as they are procedure specific. I.e., it is the choice of a procedure that defines the p-value adjustment and the confidence intervals, and not the error rate itself.


\section{\label{sec:measures_of_error}Measures of Error}

\subsection{Family Wise Error Rates}
Consider the testing of several null hypotheses against their respective research (alternative) hypotheses. The Family Wise Error Rate (FWER) is the frequency of experiments in which a false rejection of a null hypothesis will occur. I.e., the probability of a false finding, or, false positive.




Table~\ref{tab:event_notation} introduces the nomenclature which has become standard in the multiple comparisons community and will be referenced throughout this article. Following this notation, the FWER is defined as $$Prob\{V \geq1  \}$$ where $P(.)$ denotes the relative frequency within all possible experimental results.



\begin{table}[h]
  \centering
\begin{tabular}{|c|c|c|c|}
\hline \rule[-1ex]{0pt}{1.5ex} & Claimed nonsignificant & Claimed Significant & Total \\ 
\hline
\hline \rule[-1ex]{0pt}{1.5ex} Null & $U$ & $V$ & $m_0$ \\ 
\hline \rule[-1ex]{0pt}{1.5ex} Nonnull & $T$ & $S$ & $m_1$ \\ 
\hline \rule[-1ex]{0pt}{1.5ex} Total & $m-R$ & $R$ & $m$ \\ 
\hline 
\end{tabular} 
  \caption{Classification of types of decisions made}
  \label{tab:event_notation}
\end{table}


\subsubsection{Weak and Strong Control}
The probability of any particular inference procedure of making a false finding, depends on the existence of true effects. FWER control in the ``weak'' sense, refers to procedures which guarantee low FWER when there are no true effects. I.e., when all null hypotheses are correct. 
FWER control in the ``strong'' sense is the complementing concept. Referring to procedures which guarantee FWER control even in the presence of some true effects. I.e., when not all null hypotheses are correct. 
As their names imply, ``strong'' control is the stricter criterion, which entails ``weak'' control.




\subsection{\label{sub:fdr}False Discovery Rates}

Consider the same setup as in the previous section. The False Discovery Rate (FDR), first introduced by \citet{benjamini_controlling_1995}, is the average ratio between false discoveries and total discoveries over replicated experiments. 
Denoting the (unknown) False Discovery Proportion of a particular inference using a particular data set as $ FDP=V/R $, and setting the convention that no discoveries signify no errors ($R=0 \Rightarrow FDP=0$), the FDR can be now defined as:
$$E \left( FDP \right)$$ 
where $E(.)$ denotes the average over all possible experiments results.


Some remarks follow:
\begin{enumerate}
\item The FDR error rate has become synonymous with the Benjamini-Hochberg procedure presented in \citep{benjamini_controlling_1995} . This is plain wrong and confusing. Benjamini-Hochberg is a procedure that does indeed offer FDR control in particular setups, but it is only one of many.
\item In the context of FDR, there is no $\alpha=0.05$ convention. Researchers are free and welcome to choose the right level of error they see adequate for their particular research. Obviously, an error level of $\alpha=0.5$ might be hard to defend from critique.
\end{enumerate}




\subsection{Other Measures of Error}
FWER and FDR are the most commonly used, but by no means the only ones. Since many error measures are merely an average over replications of the experiment, many other error measures can be considered. Just by replacing the error function to be averaged. Denoting by $E(C)$ the average, over all possible experiments outcomes, of some error $C$. We now see that FWER and FDR simply set  $C = I_{\{ V \geq 1 \} } $ and $C = FDP$ respectively.
Some other measures of error are:

\begin{itemize}
\item Per Family Error Rate (PFER): Where $C=V$.\\
This measure is the simple (expected) number of errors. 
\item Per Comparison Error Rate (PCER): Where $C=V/n$.\item $k$-FWER \citep{van_der_laan_augmentation_2004}: Where $C(k) = I_{\{ V \geq k \} }$.
\item False Discovery Exceedance (FDX)\citep{genovese_exceedance_2006}: Where $C(\gamma) = I_{\{ V/R \geq \gamma \} }$.\\
This measure was motivated by the fact that FDR keeps the proportion of false discoveries small, but only \emph{on average}. FDR does not exclude the possibility of making more than $FDP>\alpha$ mistakes in almost all samples. FDX targets this scenario explicitly, and is thus more conservative than FDR. 
\end{itemize}

Other measures of error which are not simple averages over replications of the experiment include, but are not limited to:
\begin{itemize}

\item Positive FDR or pFDR \citep{storey_direct_2002} or $FDR_{-1}$ \citep{benjamini_discovering_2010} : Defined as $E(V/R;R>0)$.\\
This measure is essentially the proportion of false findings (within all findings), but averaged, not on all possible experiments outcomes, but only on those which actually return findings. It was motivated by the observation that the FDR of a procedure might be very low merely because in many events it returns no findings, even if it makes many mistakes when it does indeed return findings (see \citep{storey_direct_2002} for a example). 
On the other hand, if all the null hypotheses are true, thus all findings are false, one would want an error measure to coincide with the the probability of a false finding (weak FWER). pFDR does not enjoy this attribute.

\item Marginal FDR or mFDR \citep{sun_oracle_2007} or Fdr \citep{efron_microarrays_2008} or $FDR_{+1}$ \citep{benjamini_discovering_2010}: Defined as $E(V)/E(R)$. \\
While not very interesting for itself, this error measure gained popularity since it is mathematically tractable and approximates the FDR when many independent hypothesis are being tested. 

\end{itemize}


We conclude by noting that FWE, FDR, pFDR and mFDR are (currently) by far the most popular. So much so that it is actually hard to find published applied research using any other. 



\subsection{Choosing Your Family}
All the previous error measures assume that the family of hypotheses is known. As a researcher, defining the family is not an obvious task. The examples in section~\ref{sec:examples} include some trivial scenarios, in the sense that the family of hypotheses is clear. The section also includes some examples where the family is not trivial (see \ref{eg:imaging_genetics}) and its choice will depend on the scientific statement in mind.




\section{\label{sec:examples}Examples}



\subsection{Tukey's Psychological Exams}
In his 1953 unpublished paper: ``The Problem of Multiple Comparisons'' \citep{benjamini_john_2002} and later, when lecturing at Princeton University \citep{donoho_higher_2004}, Prof. John Tukey would tell the tale of a young psychologist. After administering 250 tests he finds that 11 were significant at the 0.05 level. After the initial feeling of satisfaction he consults a senior researcher, only to discover his findings are rather poor as one  would expect 12.5 significant tests due to chance alone. Having only 11 significant results is actually disappointing.


With this new understanding, our psychologist now has to decide how should he protect himself from false findings? 
Say the tests consist of new candidate clinical diagnostics for condition X. Making an error means that a test will be used to diagnose X while it actually cannot distinguish between healthy and X. Since this is unacceptable for our psychologist, he will want an inference procedure that controls the FWE in the strong sense. 
In Tukey's example, the tests are independent, so he could consider, say Holm's procedure \citep{holm_1979}.


Now consider a different scenario: The tests check for differences in personality attributes between genders. Making an error means that the psychologist might believe male and female differ in a way they actually do not. The researcher does not consider this a serious mistake, as long as many other true differences are discovered. In this setup, the researcher should control the FDR -- probably using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}. Allowing for some mistakes will allow the researcher to enjoy a sensitivity gain compared to FWER-controlling-procedures.


\subsection{ANOVA}
[Should this be included? If so, I need a good story.]



\subsection{\label{sub:fMRI}Functional Magnetic Resonance Imaging}

Consider now the case of the neuroscientist, trying to locate the brain regions responsive to visual stimuli. He will scan a dozen subjects or so in the Magnetic Resonance Imaging (MRI) machine and record the brain's activation\footnote{ He actually measures the blood oxygenation level. Details can be found in \cite{lazar_statistical_2008}.} during the stimuli. To be precise, he will be measuring the activation level at \emph{each} of several thousand brain locations, called Volumetric Picture Elements (voxels). Their exact number depending on the MRI's resolution. 
In the aim of finding ``active'' locations, the researcher will want to test the null hypothesis of no response to the stimulus, at each voxel. A mistake would mean he declared a voxels as responsive when it actually is not. This does not seem like a terrible mistake to make so the researcher should probably protect himself from large proportions of errors, and not from the making of one single error. FDR is thus the error measure of choice.


\subsubsection{Functional Magnetic Resonance Imaging-- Cluster Level Inference}
Returning to the neuroscientist from sec~\ref{sub:fMRI}-- recalling that the voxels are arbitrary volume units, defined by the technology of the MRI and not by entities of interest for inference-- he decides that a more interesting entity is a mass of contiguous activations. He will thus decide that he is interested in spatially contiguous regions with activation larger than ``7'' (in some scale). After scanning a subject, he realizes there are 30 contiguous regions which exceed 7. Knowing enough probability theory, he is able to compute a p-value for the volume of each of the 30 regions. Since he can accept false regions, as long as their proportion is not too high, he should probably use FDR control. Alas, the FDR defined in section~\ref{sub:fdr} assumes a fixed and known number of hypotheses, and the number of excursion regions is random. Extensions of the FDR for the random-hypothesis case do exist. A rigorous exposition can be found in  \citet{siegmund_false_2011}. We do note however, that error-controlling \emph{procedures} for the random hypothesis case are not as abundant and studied as the fixed hypothesis case. The mathematical proofs would typically require some hard to justify assumptions. Simulation performances however, do seem promising \citep{chumbley_false_2009,chumbley_topological_2010}.


\subsubsection{Functional Magnetic Resonance Imaging-- Clinical Scan}
Returning again to the neuroscientist from sec~\ref{sub:fMRI}-- this time his patient is about to enter surgery, for the removal of a brain tumour. The patient will be scanned in the fMRI in order to localize the speech regions, as the tumour is residing nearby and the surgeon needs to be extra-careful around these regions. This is a case where type $II$ errors are arguably more important than type $I$ errors: underestimating the speech region might cost the patient his verbal skills. Overestimating it, might cost him an extra surgery or a recurring tumour. None of the error measures presented until now adresses is concerned with false negatives. Recurring to the terminology in table~\ref{tab:event_notation}, our neuroscientist would probably be interested in something in the likes of $E(T/(m-R))$ which captures the sensitivity of the inference. This measure is the False Non Detection Rate (FNR) \citep{genovese_ROC_2002} and we shall revisit it in section~\ref{sec:power}. 




\subsection{Genome Wide Association Studies}
In a typical Genome Wide Association Study (GWAS) the geneticist will record the genetic information of many subjects (genotyping) with the aim of discovering associations between the genotype and the individuals' attributes (phenotype). Assuming a univariate phenotype, the researcher will perform some sort of regression between the phenotype and genetics attributes (titled single nucleotide polymorphism-- SNP). With today's technology, the number of SNPs considered in a typical GWAS is in the hundreds of thousands. To declare an association, a researcher will try to reject the no association hypothesis between \emph{each} SNP and the phenotype, leading to the simultaneous testing of several hundreds of thousands of hypotheses. Since the researcher does not concern himself with the making of a mistake, as long as other associations discovered as true, he will choose FDR control. Probably using the Benjamini-Hochberg procedure.




% Is another FWER example needed? Maybe ANOVA? Examples from Donoho & Jin 2004?


\subsection{\label{eg:imaging_genetics}Imaging Genetics}
The field of imaging genetics, aims at finding the genetic attributes associates with phenotypes derived from medical imaging. In a pioneering study, \citet{stein_voxelwise_2010} set out to find the genetic variation associated with local brain volume, under the paradigm that different genes affect different brain regions. The data included the genotyping and imaging of about $N \approx 700$ individuals. The genotype of each individual comprises of information about close to $n_G \approx 400K$ SNPs. The imaging data encodes the relative volume of each subject at $n_B \approx 30K$ voxels. Testing for association between all $\{SNP\} \times \{voxel\}$ combinations, leads to $n_G \cdot n_B \approx 1.2B$ hypotheses. Should they all be considered one family of hypotheses? Or maybe each SNP (or voxel) is actually a separate family? 

A researcher might want to infer which gene is associated with which location? This single family of hypotheses will include all $\{SNP\} \times \{voxel\}$ combinations. FDR control over this family, means that researcher is concerned with the proportion of false associations detected within all of the $\{SNP\} \times \{voxel\}$ associations found. Our researcher is very much aware of the fact she will have to correct for $1.2B$ hypotheses and starts considering alternative approaches. A natural option might be SNP-wise testing. This would require correcting for $n_B$ voxel in each of the $n_G$  families tested. Power will certainly be gained by this approach, as the correction is performed only for $n_B$ hypotheses at a time, but what about false findings? Sadly, these is still a non-negligible chance of many erroneous findings. To see this, consider the case where there is only one voxel: this amounts to $n_G$ level $\alpha$ hypothesis tests, which is this initial multiplicity problem. Indeed, attention has to be given to the number of families being tested. 

A possible solution might harness the hierarchy of the problem. Say, by aggregating all voxels within each SNP, then testing the $n_G$ null hypotheses of ``unassociated SNP'', and then drilling down to the voxel level only within \emph{selected} SNPs. Naturally, the multiplicity is alleviated since only selected SNPs will be be passed for voxel-wise testing. Alas, if the same data is used for selecting the SNPs and then selecting the voxels, an $\alpha$ level FDR control within selected SNPs will still not guarantee an $\alpha$ levelFDR control, across discovered $\{SNP\} \times \{voxel\}$ associations. This is actually a case of \emph{selective inference} \citep{benjamini_simultaneous_2010}, affectionately referred to by practitioners as ``data snooping'' or ``double dipping''. 

Having given it more thought- our researcher decides she wants a method that has two properties: 
(a) It controls for the number of falsely discovered SNPs. 
(b) It controls for the number of falsely discovered voxels associated with each discovered SNP. 

To be more precise, table~\ref{tab:event_notation} need refinement. Define $R$ and $V$ to be the number of discovered SNPs and falsely discovered SNPS respectively. 
Define $R_g$ to be number of voxels declared associated with SNP $g$, and $V_g$ accordingly. The desired measure of error has two requirements: 

\begin{equation} \label{eq:hirarchial_error}
 E \left(\frac{V}{R} \right)\leq \alpha_1 
\text{ and } 
E \left( \frac{1}{R}\sum_{g \in S} \frac{V_{g}}{R_{g}} \right)\leq \alpha_2
\end{equation}



Is there a procedure that controls this type of error? While novel and under active research, there is presently one such procedure\footnote{With proofs for the independent test-statistics case}. It has two stages: 
First, the omnibus-stage:  testing for an associated SNP by aggregating over voxels within SNP and controlling for the number of SNPs tested. Second, a post-hoc stage: drilling into the selected SNPs searching for associated voxels. The novelty of the procedure is at the second stage, which controls for the number of voxels with a conservative error rate, which accounts for the previous SNP selection stage. 
The details can be found in \citet{benjamini_adjusting_2011}.




\section{\label{sec:power}Power Considerations}

The reader might have noticed that the different error measures in section~\ref{sec:measures_of_error} care only of the number of false discoveries. Our interest in detection sensitivity is naturally implicit in the procedures researchers employ. Otherwise, never rejecting any null hypothesis, will trivially control all of the error types in section~\ref{sec:measures_of_error}. 
The introduction of power requires the consideration of the expected deviation from the null hypotheses. To see this, consider a scenario where the researcher is certain that if an effect exists, it would be of magnitude $\pm 7$ (in some arbitrary scale, say z-values). Ignoring this belief, will lead the researcher to reject all hypotheses with large (absolute) effects. Particularly, an effect of, say 20,  will be considered very extreme, with infinitesimally small p-values. But, when considering the fact that effects are expected to be near 7, the researcher might actually prefer to reject effects near 7 \emph{before} he rejects effects near 20. 

Specifying the expected deviation from the null for each hypothesis tested is no easy task. There exist however, several procedures which use the multitude of hypotheses tested in an attempt to empirically characterize the deviations from the null, and harness this information. The most popular rely on estimators of the probability of a hypothesis being a true null given the value of the test statistic $z_i$. This is the posterior probability of the null, also named ``local fdr'' and denoted by $fdr(z_i)$. The details can be found in \cite{efron_microarrays_2008}. Note, that these are not error measures but rather procedures relying on test statistics other than p-values.  

Using the notation presented in table~\ref{tab:event_notation}, we can specify many error measures which capture the idea of maximal power subject to a  that the false detections are kept at some desired, low level:

\begin{align}
        min\{FNR \quad s.t. \quad FDR\leq \alpha \} \label{eq:compound_1}\\
	min\{mFNR \quad s.t. \quad mFDR\leq \alpha \} \label{eq:compound_2}\\
	min\{T \quad s.t. \quad V \leq \alpha \} \label{eq:compound_3}
\end{align}


Most procedures aimed at satisfying these error functions typically use the local fdr statistic ($fdr(z_i)$) as a test statistic, but differ in the heuristics used to  compute this statistic \cite[eg.][]{storey_direct_2002,efron_microarrays_2008,sun_oracle_2007}. Typically assuming a large number of hypotheses being tested, and independence between test statistics. 

The search for procedures with desirable properties under errors measures such as eq~\ref{eq:compound_1}  - eq~\ref{eq:compound_3}, is ongoing. It is an active field of investigation with beautiful theory, either developed by the Multiple Comparisons community or borrowing from statistical decision theory \cite[see][]{sun_oracle_2007}. The analysis of the finite-sampling properties of suggested procedures with respect to some desirable error measures is not an easy task.  For a complete treatment, and state of the art procedures, the reader is referred to \cite{efron2010large}.








\section{Acknowledgments}
% Yoav
% Kornelius



\appendix

\section{\label{sec:on_your_pc} On Your Computer}
It does not suffice to choose an error measure in order to perform an analysis. An error-controlling procedure will also have to be chosen, and this is what you should look for in your favourite software. 

In the R programming environment \citep{r_development_core_team_r:_2011} the function \emph{p.adjust} in the \emph{stats} package will allow to perform the most common procedures. The references in the function documentation are a good starting point for learning about these procedures. 
For FWER controlling procedures, in particular in the context of linear contrasts in regression models, the \emph{multcomp} package is a very good option. 
For FDR control (and variants) many packages have been written. A good listing of these can be found in Korbinian Strimmer's web site: \url{http://strimmerlab.org/notes/fdr.html}. 
We also note that the hierarchical testing scheme in section~\ref{eg:imaging_genetics} has not been implemented to the best of our knowledge. Its implementation would require a new syntax to describe the hypotheses' hierarchy (families) and is an open challenge. 

In SAS, multiple testing procedures are incorporated within PROC MIXED and also in PROC MULTEST.  The canonical reference is \citet{westfall_multiple_2011}. 

In SPSS, multiplicity corrections are typically found as part of the \emph{post-hoc} options of the analysis methods.



\section{\label{sec:glossary} Glossary}
Some of the terms in the multiple-comparisons literature, have appeared in several other disciplines under different names. In this glossary, we use as a reference the statistical nomenclature in table~\ref{tab:event_notation}. Also note that we use \emph{rate} for the average of a \emph{ratio} or a \emph{proportion}. The literature is not consistent regarding this convention, so that the terms might be found in use for both purposes.

%\begin{table}[H]
%  \centering
%\begin{tabular}{|c|c|c|c|}
%\hline \rule[-1ex]{0pt}{1.5ex} & Claimed nonsignificant & Claimed Significant & Total \\ 
%\hline
%\hline \rule[-1ex]{0pt}{1.5ex} Null & $U$=True Negatives & $V$=False Positives & $m_0$ \\ 
%\hline \rule[-1ex]{0pt}{1.5ex} Nonnull & $T$=False Negatives & $S$=True Positives & $m_1$ \\ 
%\hline \rule[-1ex]{0pt}{1.5ex} Total & $m-R$=Negatives & $R$=Positives & $m$ \\ 
%\hline 
%\end{tabular} 
%  \caption{Classification of types of decisions made}
%  \label{tab:engineering_notation}
%\end{table}


\begin{description}

\item[Accuracy] $R/m$
\item[Fall-out] $V/R$
\item[False Alarm Rate (FDR)] $E(V/R)$
\item[False Alarm Ratio] $V/R$
\item[False Alarms] $V$
\item[False Detection Proportion (FDP)] $V/R$
\item[False Detection Rate (FDR)] $E(V/R)$
\item[False Discoveries] $V$
\item[False Discovery Ratio] $V/R$
\item[False Discovery Proportion (FDP)] $V/R$
\item[False Discovery Rate (FDR)] $E(V/R)$
\item[False Negatives] $T$
\item[False Non Discoveries] $T$
\item[False Non Discovery Rate (FNR)] $E(T/(m-R))$
\item[False Non Discovery Ratio] $T/(m-R)$
\item[False Positive Rate] $E(V/R)$
\item[False Positive Ratio] $V/R$
\item[False Positives] $V$
\item[Hit Rate] $E(S/R)$
\item[Hit Ratio] $S/R$
\item[Hits] $S$
\item[Misses] $T$
\item[Negative Predictive Value] $U/(m-R)$
\item[Non Discovery Rate] $E(T)/m_1$
\item[Non Discovery Ratio] $T/m_1$
\item[Positive Predictive Value (PPR)] $S/R$
\item[Precision] $S/R$
\item[Recall] $S/m_1$
\item[Specificity] $U/m_1$
\item[True Discoveries] $S$
\item[True Negative Rate] $E(U)/m_1$
\item[True Negative Ratio] $U/m_1$
\item[True Negatives] $U$
\item[True Positive Rate] $E(S/R)$
\item[True Positive Ratio] $S/m_1$
\item[True Positives] $S$
\item[Type $II$ errors] $T$
\item[Type I Errors] $V$

\end{description}





%\begin{description}
%\item[$V$:] False Positives, False Alarms, Type I Errors, False Discoveries
%\item[$T$:] False Negatives, False Non Discoveries, Misses, Type $II$ errors.
%\item[$S$:] True Positives, Hits, True Discoveries.
%\item[$U$:] True Negatives.
%\item[$S/R$:] Hit Ratio, Precision, Positive Predictive Value.
%\item[$E(S/R)$:] True Positive Rate, Hit Rate.
%\item[$S/m_1$:] Recall, True Positive Ratio
%\item[$R/m$:] Accuracy.
%\item[$U/m_1$:] Specificity, True Negative Ratio.
%\item[$E(U)/m_1$:] True Negative Rate.
%\item[$V/R$:] False Discovery Proportion, False Discovery Proportion Ratio, False Detection Proportion, False Alarm Ratio, False Positive Ratio, Fall-out. 
%\item[$E(V/R)$:] False Discovery Rate, False Detection Rate, False Alarm Rate, False Positive Rate. 
%\item[$U/(m-R)$:] Negative Predictive Value.
%\item[$T/(m-R)$:] False Non Discovery Ratio.
%\item[$E(T/(m-R))$:] False Non Discovery Rate.
%\item[$T/m_1$:] Non Discovery Ratio.
%\item[$E(T)/m_1$:] Non Discovery Rate.
%\end{description}



\bibliographystyle{plainnat}
\bibliography{PracticalGuideToMultiplicity}


\end{document}  

