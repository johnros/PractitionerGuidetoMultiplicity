\documentclass[review,12pt]{article}
%\documentclass[12pt,draft]{amsart}
%\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
%\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{amsfonts}   % if you want the fonts
\usepackage{epstopdf}
\usepackage{setspace}
\usepackage[numbers]{natbib}
%\bibpunct{[}{]}{,}{n}{}{;}
\usepackage{hyperref}
%\usepackage{glossaries}
\usepackage{float}



\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
\doublespacing
%\makeglossaries


\title{A Practitioner's Guide to Multiple Testing Error Rates}
\author{Jonathan Rosenblatt}
\date{}                                         




\begin{document}
\maketitle


\section{Introduction}

It is quite common for modern research to test many hypotheses simultaneously. The statistical (frequentist) hypothesis testing framework does not scale with the number of hypotheses in the sense that performing many hypothesis tests will probably yield many false findings. In order to scale, a researcher has to consider of the type or errors he wishes to avoid and select the adequate procedure for that particular error type and data structure. A quick search of the tag \emph{[multiple-comparisons]} in the statistics Questions \& Answers web site Cross Validates (\url{http://stats.stackexchange.com}) demonstrates the amount of confusion this task can actually cause. This was also a point made at the 2009 Multiple Comparisons conference in Tokyo \citep[][section 4.4]{benjamini_simultaneous_2010}. 
In an attempt to offer guidance, we review possible error types for simultaneous inference (sec~\ref{sec:measures_of_error}) and demonstrate them with some practical examples (sec \ref{sec:examples}), which clarify the formalism of sec~\ref{sec:measures_of_error}. Finally, we include some notes on the software implementations of the methods discussed, in appendix~\ref{sec:on_your_pc}.

The emphasis of this manuscript is on the error rates, and not on the procedures themselves. For the purpose of selecting the appropriate procedure consult your favourite software's documentation (see our appendix~\ref{sec:on_your_pc}). Alternatively, \citet{farcomeni_review_2008} can serve as a good reference. 
We do try to name several procedures in this manuscript where appropriate.  Simultaneous confidence intervals and p-value adjustment will not be discussed as they are procedure specific. I.e., it is the choice of a procedure that defines the p-value adjustment and the confidence intervals, and not the error rate itself.


\section{\label{sec:measures_of_error}Measures of Error}

\subsection{Family Wise Error Rates}
Consider the testing of several null hypotheses against their respective research (alternative) hypotheses. The Family Wise Error Rate (FWER) is the frequency of experiments in which a false rejection of a null hypothesis will occur. I.e., the probability of a false finding, or, false positive.




Table~\ref{tab:event_notation} introduces the nomenclature which has become standard in the multiple comparisons community and will be referenced throughout this article. Following this notation, the FWER is defined as $$Prob(V \geq 1 )$$ where $Prob(.)$ denotes the relative frequency within all possible experimental results.



\begin{table}[h]
  \centering
\begin{tabular}{|c|c|c|c|}
\hline \rule[-1ex]{0pt}{1.5ex} & Claimed nonsignificant & Claimed Significant & Total \\ 
\hline
\hline \rule[-1ex]{0pt}{1.5ex} Null & $U$ & $V$ & $m_0$ \\ 
\hline \rule[-1ex]{0pt}{1.5ex} Nonnull & $T$ & $S$ & $m_1$ \\ 
\hline \rule[-1ex]{0pt}{1.5ex} Total & $m-R$ & $R$ & $m$ \\ 
\hline 
\end{tabular} 
  \caption{Classification of types of decisions made}
  \label{tab:event_notation}
\end{table}


\subsubsection{Weak and Strong Control}
The probability of any particular inference procedure of making a false finding, depends on the existence of true effects. FWER control in the ``weak'' sense, refers to procedures which guarantee low FWER when there are no true effects, i.e., when all null hypotheses are correct. 
FWER control in the ``strong'' sense is the complementing concept, referring to procedures which guarantee FWER control even in the presence of some true effects. I.e., when not all null hypotheses are correct. 
As their names imply, ``strong'' control is the stricter criterion, which entails ``weak'' control.




\subsection{\label{sub:fdr}False Discovery Rates}

Consider the same setup as in the previous section. The False Discovery Rate (FDR), first introduced by \citet{benjamini_controlling_1995}, is the average ratio between false discoveries and total discoveries over replicated experiments. 
Denoting the (unknown) False Discovery Proportion (FDP) of a particular inference using a particular data set as $ FDP=V/R $, and setting the convention that no discoveries signify no errors ($R=0 \Rightarrow FDP=0$), the FDR can be now defined as:
$$E \left( FDP \right)$$ 
where $E(.)$ denotes the average over all possible experimental results.


Some remarks follow:
\begin{enumerate}
\item The FDR error rate has become synonymous with the Benjamini-Hochberg procedure presented in \citep{benjamini_controlling_1995} . This is plain wrong and confusing. Benjamini-Hochberg is a procedure that does indeed offer FDR control in particular setups, but it is only one of many.
\item In the context of FDR, there is no $\alpha=0.05$ convention. Researchers are free and welcome to choose the right level of error they see adequate for their particular research. Obviously, an error level of $\alpha=0.5$ might be hard to defend from critique.
\end{enumerate}




\subsection{Other Measures of Error}
FWER and FDR are the most commonly used, but by no means the only ones. Since many error measures are merely an average over replications of the experiment, many other error measures can be considered. Just by replacing the error function to be averaged. Denoting by $E(C)$ the average, over all possible experiments outcomes, of some error $C$. We now see that FWER and FDR simply set  $C = I_{\{ V \geq 1 \} } $ and $C = FDP$ respectively.
Some other measures of error are:



\begin{itemize}

\item Per Family Error Rate (PFER): Where $C=V$.\\
This measure is the simple (expected) number of erroneous discoveries. 

\item Per Comparison Error Rate (PCER): Where $C=V/m$.

\item $k$-FWER \citep{van_der_laan_augmentation_2004}: Where $C(k) = I_{\{ V \geq k \} }$.\\
This measure is the relative frequency of the making of no less than $k$ erroneous discoveries.

\item False Discovery Exceedance (FDX)\citep{genovese_exceedance_2006}: Where $C(\gamma) = I_{\{ V/R \geq \gamma \} }$.\\
This measure was motivated by the fact that FDR keeps the proportion of false discoveries small, but only \emph{on average}. 
In extreme scenarios, FDR does not exclude the possibility of making more than $FDP>\alpha$ mistakes in \emph{almost all} experiments. FDX targets these scenarios explicitly, and is thus more conservative than FDR. 

\end{itemize}



Other measures of error which are not simple averages over replications of the experiment include, but are not limited to:
\begin{itemize}

\item Positive FDR or pFDR \citep{storey_direct_2002} or $FDR_{-1}$ \citep{benjamini_discovering_2010} : Defined as $E(V/R;R>0)$.\\
This measure is essentially the proportion of false findings (within all findings), but averaged, not on all possible experiments outcomes, but only on those which actually return findings. It was motivated by the observation that the FDR of a procedure might be very low merely because in many events it returns no findings, even if it makes many mistakes when it does indeed return findings (see \citep{storey_direct_2002} for a example). 
On the other hand, if all the null hypotheses are true, thus all findings are false, one would want an error measure to coincide with the the probability of a false finding (weak FWER). pFDR does not enjoy this attribute.

\item Marginal FDR or mFDR \citep{sun_oracle_2007} or Fdr \citep{efron_microarrays_2008} or $FDR_{+1}$ \citep{benjamini_discovering_2010}: Defined as $E(V)/E(R)$. \\
While not very interesting for itself, this error measure gained popularity since it is mathematically tractable and approximates the FDR when many independent hypothesis are being tested. 

\end{itemize}


We conclude by noting that FWE, FDR, pFDR and mFDR are (currently) by far the most popular. So much so that it is actually hard to find published applied research using any other. 



\subsection{Choosing Your Family}
All the previous error measures assume that the family of hypotheses is known. As a researcher, defining the family is not an obvious task. The examples in section~\ref{sec:examples} include some trivial scenarios, in the sense that the family of hypotheses is clear. The section also includes some examples where the family is not trivial (see \ref{eg:imaging_genetics}) and its choice will depend on the scientific statement in mind.




\section{\label{sec:examples}Examples}



\subsection{Tukey's Psychological Exams}
In his 1953 unpublished paper: ``The Problem of Multiple Comparisons'' \citep{benjamini_john_2002} and later, when lecturing at Princeton University \citep{donoho_higher_2004}, Prof. John Tukey would tell the tale of a young psychologist. After administering 250 tests he finds that 11 were significant at the 0.05 level. After the initial feeling of satisfaction he consults a senior researcher, only to discover his findings are rather poor as one  would expect 12.5 significant tests due to chance alone. Having only 11 significant results is actually disappointing.


With this new understanding, our psychologist now has to decide how should he protect himself from false findings? 
Say the tests consist of new candidate clinical diagnostics for condition X. Making an error means that a test will be used to diagnose X while it actually cannot distinguish between healthy and X. Since this is unacceptable for our psychologist, he will want an inference procedure that controls the FWE in the strong sense. 


Now consider a different scenario: The tests check for differences in personality attributes between genders. Making an error means that the psychologist might believe male and female differ in a way they actually do not. The researcher does not consider this a serious mistake, as long as many other true differences are discovered. In this setup, the researcher should control the FDR -- probably using the Benjamini-Hochberg procedure \citep{benjamini_controlling_1995}. Allowing for some mistakes will allow the researcher to enjoy a sensitivity gain compared to FWER-controlling-procedures. The interpretation of the findings should be done in accord with the error measure employed.


\subsection{ANOVA}
[Should this be included? If so, I need a good story.]



\subsection{\label{sub:fMRI}Functional Magnetic Resonance Imaging}

Consider now the case of the neuroscientist, trying to locate the brain regions responsive to visual stimuli. He scaned a dozen subjects or so in the Magnetic Resonance Imaging (MRI) machine and recorded the brain's activation\footnote{ He actually measured the blood oxygenation level. Details can be found in \cite{lazar_statistical_2008}.} in response to the stimuli. To be precise, he measured the activation level at \emph{each} of several thousand brain locations, called Volumetric Picture Elements (voxels). Their exact number depending on the resolution of the MRI scan . With the measured activation levels at hand, the researcher can compute their correlation to the stimulus given. If the voxel-wise measurement is correlated with the stimulus, the location is considered ``active''. We see that localising activation actually consists of performing many local hypothesis tests: the null hypothesis of no correlation to the stimulus is tested at each voxel.

Returning to multiplicity error rates; An error would mean he declared a voxel as responsive when it actually is not. This does not seem like a terrible mistake to make so the researcher should probably protect himself from large proportions of errors, and not from the making of one single error. FWER, k-FWER and Per Family Error Rate are thus excluded. Per Comparison Error Rate seems like a possible candidate, but it very liberal. One can actually gain power by including many ``junk'' hypotheses. To see this consider the case of infinitely many hypotheses tested. The proportion of errors will trivially be smaller than any $\alpha$ we pick. Our researcher is thus left with FDR and FDX as candidate error measures. Finally, FDR is chosen due to the plethora of procedures available to control it. 


\subsubsection{Functional Magnetic Resonance Imaging-- Cluster Level Inference}

Returning to the neuroscientist from sec~\ref{sub:fMRI}-- recalling that the voxels are arbitrary volume units, defined by the technology of the MRI and not by entities of interest for inference-- he decides that a more interesting entity is a mass of contiguous activations. He thus decides that he is interested in spatially contiguous regions with activation larger than ``7'' (in some scale). These regions are known as ``excursion regions'', ``excellence sets'', ``blobs'' and possibly many other names. 
After scanning a subject, he realises there are 30 contiguous regions which exceed 7. Knowing enough probability theory, he is able to compute a p-value for the volume of each of the 30 regions. If he rather not make any mistakes, he can control the FWER of the regions. Namely, controlling the probability of declaring any inactive regions as active. This is indeed the approach implemented in several brain analysis software packages, particularly SPM (\url{http://www.fil.ion.ucl.ac.uk/spm/}).

Alternatively, if the researcher can allow for some slack and accept false regions-- as long as their proportion is not too high-- he should probably use FDR control. Alas, the FDR defined in section~\ref{sub:fdr} assumes an a priori fixed and known number of hypotheses being tested ($m$). The number of excursion regions is data dependent, thus random and a priori unknown. Extensions of the FDR for the random-number-of-hypothesis case do exist. A rigorous exposition can be found in  \citet{siegmund_false_2011}. We do note however, that error-controlling \emph{procedures} for the random hypothesis case are not as abundant and studied as the fixed hypothesis case. The mathematical proofs would typically require some hard to justify assumptions. Simulation performances however, do seem promising \citep{chumbley_false_2009,chumbley_topological_2010}.


\subsubsection{Functional Magnetic Resonance Imaging-- Clinical Scan}
Returning again to the neuroscientist from sec~\ref{sub:fMRI}-- this time his patient is about to enter surgery, for the removal of a brain tumour. The patient will be scanned in the fMRI in order to localize the speech regions, as the tumour is residing nearby and the surgeon needs to be extra-careful around these regions. This is a case where type $II$ errors are arguably more important than type $I$ errors: underestimating the speech region might cost the patient his verbal skills. Overestimating it, might cost him an extra surgery or a recurring tumour. None of the error measures presented until now adresses is concerned with false negatives. Recurring to the terminology in table~\ref{tab:event_notation}, our neuroscientist would probably be interested in something in the likes of $E(T/(m-R))$ which captures the sensitivity of the inference. This measure is the False Non Detection Rate (FNR) \citep{genovese_operating_2002}. We have not presented this measure yet, as it is concerned with the \emph{non-detections}. We shall revisit it in the context of power in section~\ref{sec:power}. 




\subsection{Genome Wide Association Studies}
In a typical Genome Wide Association Study (GWAS) the geneticist will record the genetic information of many subjects (genotyping) with the aim of discovering associations between the genotype and the individuals' attributes (phenotype). Assuming a univariate phenotype, the researcher will perform some sort of regression between the phenotype and genetics attributes (titled single nucleotide polymorphism-- SNP). With today's technology, the number of SNPs considered in a typical GWAS is in the hundreds of thousands. To declare an association, a researcher will try to reject the no association hypothesis between \emph{each} SNP and the phenotype, leading to the simultaneous testing of several hundreds of thousands of hypotheses. 
Since the researcher does not concern himself with the making of a mistake, as long as other associations discovered as true, he should choose FDR control. 
Having said that, we should remark that it is also very common in GWAS, to use a p-value threshold of $10^{-7}$. This threshold is intended for FWER control, when searching over $500,000$ SNPs and using the Bonferonni procedure \cite{bush_chapter_2012}. 

So FDR or FWER? Well, it is left for the researcher to decide, and ultimately depends on the implications of declaring false associations. 



% Is another FWER example needed? Maybe ANOVA? Examples from Donoho & Jin 2004?


\subsection{\label{eg:imaging_genetics}Imaging Genetics}

The field of imaging genetics, aims at finding the genetic attributes associates with phenotypes derived from medical imaging. In a pioneering study, \citet{stein_voxelwise_2010} set out to find the genetic variation associated with local brain volume, under the paradigm that different genes affect different brain regions. 
The data included the genotyping and imaging of about $N \approx 700$ individuals. 
The genotype of each individual comprises of information about close to $n_G \approx 400K$ SNPs. 
The imaging data encodes the relative volume of each subject at $n_B \approx 30K$ voxels. 
Testing for association between all $\{SNP\} \times \{voxel\}$ combinations, leads to $n_G \cdot n_B \approx 1.2B$ hypotheses. Should they all be considered one family of hypotheses? Or maybe each SNP (or voxel) is actually a separate family? 

A researcher might surely wants to infer which gene is associated with which location? 
A single family of hypotheses will include all $\{SNP\} \times \{voxel\}$ combinations. 
FWER control over this family is out of the question. 
FDR control, means that researcher is concerned with the proportion of false associations detected within all of the $\{SNP\} \times \{voxel\}$ associations found. This seems like a good criterion, except for the fact it also requires correcting for $1.2B$ hypotheses. 
Our researcher starts considering alternative error criteria. 
A natural option might be SNP-wise testing. Say, using the B-H procedure over all voxels within each SNP. Power is certainly gained as each family is corrected only for the $n_B$ voxels in it. 
What about false findings? Sadly, this approach offers no error control. To see this, consider the case where there is only one voxel: this amounts to $n_G$ level $\alpha$ hypothesis tests, which is this initial multiplicity problem. 

A more civilised solution might harness the hierarchy of the problem. Say, by selecting associated SNPs and localizing the association only for these selected SNPs. 
Naturally, the multiplicity is alleviated since only selected SNPs will be be passed for voxel-wise testing. Alas, if the same data is used for selecting the SNPs and then selecting the voxels, an $\alpha$ level FDR control within selected SNPs will still not guarantee an $\alpha$ level FDR control, across discovered $\{SNP\} \times \{voxel\}$ associations. This is actually a case of \emph{selective inference} \citep{benjamini_simultaneous_2010}, affectionately referred to by practitioners as ``data snooping'' or ``double dipping''. 

Having given it more thought, our researcher decides she wants a method that has two properties: 
(a) It controls for the number of falsely discovered SNPs. 
(b) It controls for the number of falsely discovered voxels associated with each discovered SNP. 

To put if formally, table~\ref{tab:event_notation} needs refinement. 
Define $R$ and $V$ to be the number of discovered SNPs and \emph{falsely} discovered SNPS respectively. 
Define $R_g$ to be number of voxels declared associated with SNP $g$, and $V_g$ accordingly. The desired measure of error has two requirements: 

\begin{equation} \label{eq:hirarchial_error}
 E \left(\frac{V}{R} \right)\leq \alpha_1 
\text{ and } 
E \left( \frac{1}{R}\sum_{g} \frac{V_{g}}{R_{g}} \right)\leq \alpha_2
\end{equation}


Is there a procedure that controls this type of error? While novel and under active research, there is presently one such procedure\footnote{With proofs for the independent test-statistics case}. It has two stages: 
First, the omnibus-stage:  testing for an associated SNP by aggregating over voxels within SNP and controlling for the number of SNPs tested. Second, a post-hoc stage: drilling into the selected SNPs searching for associated voxels. The novelty of the procedure is at the second stage, which controls for the number of voxels with a conservative error rate, which accounts for the previous SNP selection stage. 
The details can be found in \citet{benjamini_adjusting_2011}.




\section{\label{sec:power}Power Considerations}

The reader might have noticed that the different error measures in section~\ref{sec:measures_of_error} care only of the number of false discoveries. Our interest in detection sensitivity is naturally implicit in the procedures researchers employ. Otherwise, never rejecting any null hypothesis, will trivially control all of the error types in section~\ref{sec:measures_of_error}. 
The introduction of power requires the consideration of the expected deviation from the null hypotheses. To see this, consider a scenario where the researcher is certain that if an effect exists, it would be of magnitude $\pm 7$ (in some arbitrary scale, say z-values). Performing a one sample $z$ or $t$ test, while ignoring this belief, will lead the researcher to reject all hypotheses with large (absolute) effects. Particularly, an effect of, say 20,  will be considered very extreme, with infinitesimally small p-values. But, when considering the fact that effects are expected to be near 7, the researcher might actually prefer to reject effects near 7 \emph{before} he rejects effects near 20. In the statistical terminology, this is simply an underpowered test constructed for the wrong alternative hypothesis. 

Specifying the expected deviation from the null for each hypothesis tested is no easy task. There exist however, several procedures which use the multitude of hypotheses tested in an attempt to empirically characterise the deviations from the null~\footnote{Under mild assumptions regarding the form these deviations might take.}, and harness this information to gain power. Essentially all rely on estimators of the probability of a hypothesis being a true null given the value of some test statistic $z_i$. This is the posterior probability of the null, also named ``local fdr'' and denoted by $fdr(z_i)$. The details can be found in \cite{efron_microarrays_2008}. Note, that this is not an error measures but rather a test statistic.  

Using the notation presented in table~\ref{tab:event_notation}, we can specify many error measures which capture the idea of maximal power subject to the false detections being kept at a low level:

\begin{align}
        min\{FNR \quad \text{such that} \quad FDR\leq \alpha \} \label{eq:compound_1}\\
	min\{mFNR \quad \text{such that} \quad mFDR\leq \alpha \} \label{eq:compound_2}\\
	min\{T \quad \text{such that} \quad V \leq \alpha \} \label{eq:compound_3}
\end{align}


The different procedures aimed at satisfying these error functions typically use the local fdr statistic ($fdr(z_i)$) as a test statistic, but differ in the heuristics used to  compute this statistic \cite[eg.][]{storey_direct_2002,efron_microarrays_2008,sun_oracle_2007}. Typically assuming a large number of hypotheses being tested, and independence between test statistics. 

The search for procedures with desirable properties under errors measures such as eq~\ref{eq:compound_1}  - eq~\ref{eq:compound_3}, is ongoing. It is an active field of investigation with beautiful theory, either developed by the Multiple Comparisons community or borrowing from statistical decision theory \cite[see][]{sun_oracle_2007}. The analysis of the finite-sampling properties of suggested procedures with respect to some desirable error measures is not an easy task.  For a complete treatment, and state of the art procedures, the reader is referred to \cite{efron_large-scale_2010}.








\section{Acknowledgments}
% Yoav
% Kornelius
% David
% Neomi


\appendix

\section{\label{sec:on_your_pc} On Your Computer}
It does not suffice to choose an error measure in order to perform an analysis. An error-controlling procedure will also have to be chosen, and this is what you should look for in your favourite software. This might be a general purpose statistical suite, or a problem-specific application. In the latter case, we have little to suggest, as domain specific applications typically implement the procedures popularised in that field. 
In the brain imaging example, popular software include SPM, Brain Voyager, FSL, AFNII. All incorporate the multiplicity control procedure preferred by their authors (thus implicitly, the error measure). 
In the GWAS example, the same occurs in software such as Plink, PRESTO, PERMORY and others. 
General purpose statistical software are built for flexibility in the analysis, and thus incorporate more multiplicity control procedures. 

In the R programming environment \citep{r_development_core_team_r:_2011} the function \emph{p.adjust} in the \emph{stats} package will allow to perform the most common procedures. The references in the function documentation are a good starting point for learning about these procedures. 
For FWER controlling procedures, in particular in the context of linear contrasts in regression models, the \emph{multcomp} package is a very good option. 
For FDR control (and variants) many packages have been written. A good listing of these can be found in Korbinian Strimmer's web site: \url{http://strimmerlab.org/notes/fdr.html}. 
We also note that the hierarchical testing scheme in section~\ref{eg:imaging_genetics} has not been implemented to the best of our knowledge. Its implementation would require a new syntax to describe the hypotheses' hierarchy (families) and is an open challenge. 

In SAS, multiple testing procedures are incorporated within PROC MIXED and also in PROC MULTEST.  The canonical reference is \citet{westfall_multiple_2011}. 

In SPSS, multiplicity corrections are typically found as part of the \emph{post-hoc} options of the analysis methods.



\section{\label{sec:glossary} Glossary}
Some of the terms in the multiple-comparisons literature, have appeared in several other disciplines under different names. To ease the transition, and for completeness, we suggest a glossary. 
In this glossary, we use as a reference the statistical nomenclature in table~\ref{tab:event_notation}. Also note that we use \emph{rate} for the average of a \emph{ratio} or a \emph{proportion}. The literature is not consistent regarding this convention, so that the terms might be found in use for both purposes.

%\begin{table}[H]
%  \centering
%\begin{tabular}{|c|c|c|c|}
%\hline \rule[-1ex]{0pt}{1.5ex} & Claimed nonsignificant & Claimed Significant & Total \\ 
%\hline
%\hline \rule[-1ex]{0pt}{1.5ex} Null & $U$=True Negatives & $V$=False Positives & $m_0$ \\ 
%\hline \rule[-1ex]{0pt}{1.5ex} Nonnull & $T$=False Negatives & $S$=True Positives & $m_1$ \\ 
%\hline \rule[-1ex]{0pt}{1.5ex} Total & $m-R$=Negatives & $R$=Positives & $m$ \\ 
%\hline 
%\end{tabular} 
%  \caption{Classification of types of decisions made}
%  \label{tab:engineering_notation}
%\end{table}


\begin{description}

\item[Accuracy] $R/m$
\item[Fall-out] $V/R$
\item[False Alarm Rate (FDR)] $E(V/R)$
\item[False Alarm Ratio] $V/R$
\item[False Alarms] $V$
\item[False Detection Proportion (FDP)] $V/R$
\item[False Detection Rate (FDR)] $E(V/R)$
\item[False Discoveries] $V$
\item[False Discovery Ratio] $V/R$
\item[False Discovery Proportion (FDP)] $V/R$
\item[False Discovery Rate (FDR)] $E(V/R)$
\item[False Negatives] $T$
\item[False Non Discoveries] $T$
\item[False Non Discovery Rate (FNR)] $E(T/(m-R))$
\item[False Non Discovery Ratio] $T/(m-R)$
\item[False Positive Rate] $E(V/R)$
\item[False Positive Ratio] $V/R$
\item[False Positives] $V$
\item[Hit Rate] $E(S/R)$
\item[Hit Ratio] $S/R$
\item[Hits] $S$
\item[Misses] $T$
\item[Negative Predictive Value] $U/(m-R)$
\item[Non Discovery Rate] $E(T)/m_1$
\item[Non Discovery Ratio] $T/m_1$
\item[Positive Predictive Value (PPR)] $S/R$
\item[Precision] $S/R$
\item[Recall] $S/m_1$
\item[Specificity] $U/m_1$
\item[True Discoveries] $S$
\item[True Negative Rate] $E(U)/m_1$
\item[True Negative Ratio] $U/m_1$
\item[True Negatives] $U$
\item[True Positive Rate] $E(S/R)$
\item[True Positive Ratio] $S/m_1$
\item[True Positives] $S$
\item[Type $I$ Errors] $V$
\item[Type $II$ Errors] $T$


\end{description}





%\begin{description}
%\item[$V$:] False Positives, False Alarms, Type I Errors, False Discoveries
%\item[$T$:] False Negatives, False Non Discoveries, Misses, Type $II$ errors.
%\item[$S$:] True Positives, Hits, True Discoveries.
%\item[$U$:] True Negatives.
%\item[$S/R$:] Hit Ratio, Precision, Positive Predictive Value.
%\item[$E(S/R)$:] True Positive Rate, Hit Rate.
%\item[$S/m_1$:] Recall, True Positive Ratio
%\item[$R/m$:] Accuracy.
%\item[$U/m_1$:] Specificity, True Negative Ratio.
%\item[$E(U)/m_1$:] True Negative Rate.
%\item[$V/R$:] False Discovery Proportion, False Discovery Proportion Ratio, False Detection Proportion, False Alarm Ratio, False Positive Ratio, Fall-out. 
%\item[$E(V/R)$:] False Discovery Rate, False Detection Rate, False Alarm Rate, False Positive Rate. 
%\item[$U/(m-R)$:] Negative Predictive Value.
%\item[$T/(m-R)$:] False Non Discovery Ratio.
%\item[$E(T/(m-R))$:] False Non Discovery Rate.
%\item[$T/m_1$:] Non Discovery Ratio.
%\item[$E(T)/m_1$:] Non Discovery Rate.
%\end{description}



\bibliographystyle{plainnat}
\bibliography{PracticalGuideToMultiplicity}


\end{document}  

